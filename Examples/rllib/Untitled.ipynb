{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import run_experiments, grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--stop STOP] [--run RUN]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/b1017579/Library/Jupyter/runtime/kernel-8f41b4da-9001-41cc-81b4-03d9e0f14641.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The two-step game from QMIX: https://arxiv.org/pdf/1803.11485.pdf\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from gym.spaces import Tuple, Discrete\n",
    "\n",
    "import ray\n",
    "from ray.tune import register_env, run_experiments, grid_search\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--stop\", type=int, default=50000)\n",
    "parser.add_argument(\"--run\", type=str, default=\"QMIX\")\n",
    "\n",
    "\n",
    "class TwoStepGame(MultiAgentEnv):\n",
    "    action_space = Discrete(2)\n",
    "\n",
    "    # Each agent gets a separate [3] obs space, to ensure that they can\n",
    "    # learn meaningfully different Q values even with a shared Q model.\n",
    "    observation_space = Discrete(6)\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return {\"agent_1\": self.state, \"agent_2\": self.state + 3}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        if self.state == 0:\n",
    "            action = action_dict[\"agent_1\"]\n",
    "            assert action in [0, 1], action\n",
    "            if action == 0:\n",
    "                self.state = 1\n",
    "            else:\n",
    "                self.state = 2\n",
    "            global_rew = 0\n",
    "            done = False\n",
    "        elif self.state == 1:\n",
    "            global_rew = 7\n",
    "            done = True\n",
    "        else:\n",
    "            if action_dict[\"agent_1\"] == 0 and action_dict[\"agent_2\"] == 0:\n",
    "                global_rew = 0\n",
    "            elif action_dict[\"agent_1\"] == 1 and action_dict[\"agent_2\"] == 1:\n",
    "                global_rew = 8\n",
    "            else:\n",
    "                global_rew = 1\n",
    "            done = True\n",
    "\n",
    "        rewards = {\"agent_1\": global_rew / 2.0, \"agent_2\": global_rew / 2.0}\n",
    "        obs = {\"agent_1\": self.state, \"agent_2\": self.state + 3}\n",
    "        dones = {\"__all__\": done}\n",
    "        infos = {}\n",
    "        return obs, rewards, dones, infos\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    grouping = {\n",
    "        \"group_1\": [\"agent_1\", \"agent_2\"],\n",
    "    }\n",
    "    obs_space = Tuple([\n",
    "        TwoStepGame.observation_space,\n",
    "        TwoStepGame.observation_space,\n",
    "    ])\n",
    "    act_space = Tuple([\n",
    "        TwoStepGame.action_space,\n",
    "        TwoStepGame.action_space,\n",
    "    ])\n",
    "    register_env(\n",
    "        \"grouped_twostep\",\n",
    "        lambda config: TwoStepGame(config).with_agent_groups(\n",
    "            grouping, obs_space=obs_space, act_space=act_space))\n",
    "\n",
    "    if args.run == \"QMIX\":\n",
    "        config = {\n",
    "            \"sample_batch_size\": 4,\n",
    "            \"train_batch_size\": 32,\n",
    "            \"exploration_final_eps\": 0.0,\n",
    "            \"num_workers\": 0,\n",
    "            \"mixer\": grid_search([None, \"qmix\", \"vdn\"]),\n",
    "        }\n",
    "    elif args.run == \"APEX_QMIX\":\n",
    "        config = {\n",
    "            \"num_gpus\": 0,\n",
    "            \"num_workers\": 2,\n",
    "            \"optimizer\": {\n",
    "                \"num_replay_buffer_shards\": 1,\n",
    "            },\n",
    "            \"min_iter_time_s\": 3,\n",
    "            \"buffer_size\": 1000,\n",
    "            \"learning_starts\": 1000,\n",
    "            \"train_batch_size\": 128,\n",
    "            \"sample_batch_size\": 32,\n",
    "            \"target_network_update_freq\": 500,\n",
    "            \"timesteps_per_iteration\": 1000,\n",
    "        }\n",
    "    else:\n",
    "        config = {}\n",
    "\n",
    "    ray.init()\n",
    "    run_experiments({\n",
    "        \"two_step\": {\n",
    "            \"run\": args.run,\n",
    "            \"env\": \"grouped_twostep\",\n",
    "            \"stop\": {\n",
    "                \"timesteps_total\": args.stop,\n",
    "            },\n",
    "            \"config\": config,\n",
    "        },\n",
    "    })\n",
    "    \n",
    "    %tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
